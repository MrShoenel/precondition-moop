---
title: "Pre-conditioning of multi-objective optimization problems (problem 2)"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
papersize: a4
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{interval}
- \usepackage{xurl}
- \usepackage[nottoc]{tocbibind}
---

\intervalconfig{
    soft open fences
}

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\norm}[1]{\left\lVert\,#1\,\right\rVert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
```

```{r echo=FALSE}
source(file = "../helpers.R")
```


# Introduction to problem 2

We will try another problem, but this time without the errors we made in the first problem, also avoiding all of the pitfalls.
In this problem we will demonstrate that finding more accurate trade-offs shall only be possible using a pre-conditioned version of the chosen problem.
Before attempting this, we will follow every trick in the book to finding a desired and Pareto optimal solution to the chosen problem. That includes:

* Finding ideal- and nadir-vectors. We will do this using a global search to be most certain having found them. These vectors will be used to __scale__ each objective value, as required by the _weigted metrics_ approach we will be using [@zeleny1973compromise].
* Use a scalarizer that is guaranteed to find Pareto optimal solutions and avoids the shortcomings of the simple weighting method. We will use the augmented Chebyshev scalarizer in this problem for that. Also, we will actually test that each solution obtained is Pareto optimal, using the previously approximated Pareto front. Optimality may also be checked using, e.g., the KKT conditions.

An issue in Problem #1 was that we did not check the obtained trade-offs correctly. This must _always_ be done by obtaining the CDF's value of the loss' value.
As a first step, we will therefore approximate high-precision ECDFs for each objective, that are only used to obtain the scores later, which will be used to quantify the trade-offs. These ECDFs will __not__ be used in the pre-conditioned version of the problem. We will approximate much lower-resolution ones for that purpose.


## Todo list

- Non-linear scaling of Pareto front using scores
- orderable solutions: request some trade-offs and then order them by their total score (this is perhaps bogus because all solutions are equally good).


# The Viennet function

The MOOP we will attempt to find precise trade-offs for is the Viennet function [@viennet1996]. It is defined as in \eqref{eq:viennet}:


\begin{align}
  \min_{x,y\in\mathbb{R}}\,&\begin{cases}
    f_1(x,y)&=0.5\left(x^2+y^2\right) + \sin{\left(x^2+y^2\right)},
    \\[1em]
    f_2(x,y)&=\frac{(3x-2y+4)^2}{8}+\frac{(x-y+1)^2}{27}+15,
    \\[1ex]
    f_3(x,y)&=\frac{1}{x^2+y^2+1}-1.1\exp{\left(-x^2-y^2\right)},
  \end{cases}\label{eq:viennet}
  \\[1ex]
  \text{subject to }&-4\leq x,y\leq4\nonumber.
\end{align}


Let's define the Viennet functions and the bounds in code:

```{r}
lower <- -4
upper <- 4

ex2_f1 <- function(x, y) {
  .5 * (x^2 + y^2) + sin(x^2 + y^2)
}
ex2_f2 <- function(x, y) {
  1/8 * (3*x - 2*y + 4)^2 + 1/27 * (x - y + 1)^2 + 15
}
ex2_f3 <- function(x, y) {
  1 / (x^2 + y^2 + 1) - 1.1 * exp(-x^2 -y^2)
}
```



## Pareto front of original problem

```{r}
set.seed(1001)
constellations <- 5e4

weight_grid <- data.frame(
  w1 = runif(n = constellations),
  w2 = runif(n = constellations),
  w3 = runif(n = constellations))

ex2_pareto_front_original <- loadResultsOrCompute(file = "../results/ex2_pareto_front_original.rds", computeExpr = {
  doWithParallelCluster(numCores = 15, expr = {
    library(foreach)
    
    foreach::foreach(
      idx = 1:nrow(weight_grid),
      .inorder = FALSE,
      .combine = rbind
    ) %dopar% {
      set.seed(idx)
      w <- as.numeric(weight_grid[idx,])
      
      res <- nloptr::nloptr(
        x0 = runif(2, min = -4, max = 4),
        opts = list(
          maxeval = 2e4,
          algorithm = "NLOPT_GN_DIRECT_L_RAND"),
        eval_f = function(x) {
          w[1] * ex2_f1(x[1], x[2]) + w[2] * ex2_f2(x[1], x[2]) + w[3] * ex2_f3(x[1], x[2])
        },
        lb = c(-4, -4),
        ub = c(4, 4))
      
      s <- res$solution
      c(w, ex2_f1(s[1], s[2]), ex2_f2(s[1], s[2]), ex2_f3(s[1], s[2]), res$objective)
    }
  })
})
```

```{r}
rgl::plot3d(x = ex2_pareto_front_original[, 4], y = ex2_pareto_front_original[, 5], z = ex2_pareto_front_original[, 6])
```




# Ideal- and nadir-vectors

We will use global optimization to get a precise idea of either vector, so that we can scale the problem into the range $\interval{0}{1}$.
The ideal- \eqref{eq:vec-ideal} and nadir-vectors \eqref{eq:vec-nadir} are defined as follows:

\begin{align}
  \mathbf{z}^{\star}&=\inf_{\mathbf{x}^\star\in\mathbf{X}^\star}\,\mathbf{f}(\mathbf{x}^\star),\label{eq:vec-ideal}
  \\[1ex]
  \mathbf{z}^{\mathit{nad}}&=\sup_{\mathbf{x}^\star\in\mathbf{X}^\star}\,\mathbf{f}(\mathbf{x}^\star).\label{eq:vec-nadir}
\end{align}


In other words, we can maximize (minimize) each objective separately in order to find the components of the nadir- and ideal-vectors.
Let's define a function for that, so we can reuse it.

```{r}
ex2_nadir_ideal_approx <- function(objective, x0, minimazation = TRUE, lb = rep(lower, length(x0)), ub = rep(upper, length(x0))) {
  fac <- if (minimazation) 1 else -1
  res <- nloptr::nloptr(
    x0 = x0,
    opts = list(
      maxeval = 3e5,
      algorithm = "NLOPT_GN_DIRECT_L_RAND"),
    eval_f = function(x) {
      fac * objective(x[1], x[2])
    },
    lb = lb,
    ub = ub)
  res$objective <- fac * res$objective
  res
}
```


## Computing the vectors

Let's compute the vectors. We will use a gradient-free global search and allow up to $300,000$ iterations for each minimization/maximization.

```{r}
temp.grid <- expand.grid(list(
  Obj = c("ex2_f1", "ex2_f2", "ex2_f3"),
  Min = c(TRUE, FALSE)
))

ex2_nadir_ideal <- loadResultsOrCompute(file = "../results/ex2_nadir_ideal.rds", computeExpr = {
  doWithParallelCluster(numCores = nrow(temp.grid), expr = {
    library(foreach)
    
    foreach::foreach(
      idx = 1:nrow(temp.grid),
      rn = rownames(temp.grid),
      .inorder = FALSE,
      .combine = rbind,
      .export = levels(temp.grid$Obj)
    ) %dopar% {
      set.seed(idx)
      hp <- temp.grid[idx,]
      res <- ex2_nadir_ideal_approx(objective = get(as.character(hp$Obj)), x0 = runif(2, lower, upper), minimazation = hp$Min)
      cbind(hp, data.frame(Val = res$objective))
    }
  })
})
```

```{r echo=FALSE}
if (interactive()) {
  ex2_nadir_ideal
} else {
  knitr::kable(
    x = ex2_nadir_ideal,
    booktabs = TRUE,
    caption = "The nadir- and ideal-vectors for the three objectives of the Viennet problem.",
    label = "ex2-nadir-ideal"
  )
}
```


The result is shown in table \ref{tab:ex2-nadir-ideal}.
Let's make the two vectors out of it that are more usable:

```{r}
temp <- ex2_nadir_ideal[ex2_nadir_ideal$Min, ]
temp <- temp[order(temp$Obj),]
(ex2_ideal <- `names<-`(x = temp$Val, value = temp$Obj))
```
```{r}
temp <- ex2_nadir_ideal[!ex2_nadir_ideal$Min, ]
temp <- temp[order(temp$Obj),]
(ex2_nadir <- `names<-`(x = temp$Val, value = temp$Obj))
```

## Re-scaling the objectives

With the nadir- and ideal-vectors, we can redefine our objectives and scale them into the range $\interval{0}{1}$.
Scaling should be done using the __utopian__ vector ($\mathbf{z}^{\star\star}$), which we do not have. It is however often suggested to define it as $\mathbf{z}^\star=\mathbf{z}^{\star}-\epsilon$, where $\epsilon$ is a small constant $>0$.

As the utopian vector dominates all Pareto optimal solutions, we use it instead of the ideal vector to avoid dividing by zero in all cases. [@miettinen2008].
The re-scaling means that we have to create an utopian vector by subtracting $\epsilon$ from the ideal vector.

```{r}
ex2_epsilon <- 0.1
(ex2_utop <- ex2_utop <- ex2_ideal - ex2_epsilon)
```


Let's define the re-scaled objectives. Note we will also change the arguments from $x,y$ to a vector of simply $x$!

```{r}
ex2_f1_scaled <- function(x) {
  ex2_f1(x[1], x[2]) / (ex2_nadir[1] - ex2_utop[1])
}
ex2_f2_scaled <- function(x) {
  ex2_f2(x[1], x[2]) / (ex2_nadir[2] - ex2_utop[2])
}
ex2_f3_scaled <- function(x) {
  ex2_f3(x[1], x[2]) / (ex2_nadir[3] - ex2_utop[3])
}
```


Some scalarizers expect all objectives to have a uniform, dimensionless scale. By dividing each $f_i$ by $z_i^{\mathit{nad}}-z_i^{\star\star}$, its range will become $\interval{a}{b}$, where $b-a\approx1$ (a little less than $1$ since we're dividing by the range that is larger by $\epsilon$).

This expectation also means that we need to re-define the scaled versions of the nadir-, ideal-, and utopian-vectors after the scaling.
Since we are now in a uniform scale, I suggest reducing $\epsilon$ to $0.001$.

```{r}
ex2_epsilon_scaled <- 1e-3
(ex2_nadir_scaled <- ex2_nadir / (ex2_nadir - ex2_utop))
(ex2_ideal_scaled <- ex2_ideal / (ex2_nadir - ex2_utop))
(ex2_utop_scaled <- ex2_ideal_scaled - ex2_epsilon_scaled)
```


## Re-scaling the objective again

Let's do an intuitive way of re-scaling, by translating and scaling each objective into the interval $\interval{0}{1}$, using the nadir- and ideal-vectors only.

```{r}
ex2_f1_scaled <- function(x) {
  (ex2_f1(x[1], x[2]) - ex2_ideal[1]) / (ex2_nadir[1] - ex2_ideal[1])
}
ex2_f2_scaled <- function(x) {
  (ex2_f2(x[1], x[2]) - ex2_ideal[2]) / (ex2_nadir[2] - ex2_ideal[2])
}
ex2_f3_scaled <- function(x) {
  (ex2_f3(x[1], x[2]) - ex2_ideal[3]) / (ex2_nadir[3] - ex2_ideal[3])
}
```


Since all objectives are now minimally $0$ and maximally $1$, we define the corresponding utopian vector as ideal minus $\epsilon$:

```{r}
(ex2_utop_scaled <- rep(0, 3) - ex2_epsilon)
```




# Pareto front

Since we have three objectives, the front will be in three dimensions, too.
We will choose a number of random weight constellations, optimize each, and then obtain an approximate front.
We will be using the scaled objectives and vectors for this task.


## Scalarizer

In order to guarantee to obtain Pareto optimal solutions and to be able to avoid weakly Pareto optimal solutions, we will use a so-called augmented Chebyshev problem \eqref{eq:aug-chebyshev}.

\begin{align}
  \mathrm{minimize}\,\overbrace{\max_{i=1,\dots,k}{\left[w_i(f_i(\mathbf{x})-z_i^{\star\star})\right]}}^{\text{scalarizer}} + \overbrace{\rho\sum_{i=1}^{k}\,(f_i(\mathbf{x})-z_i^{\star\star})}^{\text{augmentation term}}\label{eq:aug-chebyshev}.
\end{align}

In code, the scalarizer is defined as:

```{r}
aug_chebyshev <- function(objectives = list(ex2_f1_scaled, ex2_f2_scaled, ex2_f3_scaled), utop_vec, w_vec, x, rho = 0.01) {
  objVals <- unlist(lapply(X = objectives, FUN = function(obj) obj(x)))
  
  scalarizer <- max(w_vec * (objVals - utop_vec))
  slope <- rho + sum(objVals - utop_vec)
  
  scalarizer + slope
}
```

Let's do a test:

```{r}
set.seed(1)
aug_chebyshev(utop_vec = ex2_utop_scaled, w_vec = runif(3), x = runif(2, lower, upper))
```



## Computing the front



```{r}
set.seed(1001)
constellations <- 5e3

weight_grid <- data.frame(
  w1 = runif(n = constellations),
  w2 = runif(n = constellations),
  w3 = runif(n = constellations))
```




```{r}
ex2_pareto_front <- loadResultsOrCompute(file = "../results/ex2_pareto_front.rds", computeExpr = {
  res <- as.data.frame(doWithParallelCluster(numCores = 15, expr = {
    library(foreach)
    
    foreach::foreach(
      idx = 1:nrow(weight_grid),
      .combine = rbind,
      .inorder = FALSE
    ) %dopar% {
      w <- as.numeric(weight_grid[idx,])
      set.seed(seed = idx)
      
      res <- nloptr::nloptr(
        x0 = runif(n = 2, min = lower, max = upper),
        opts = list(
          maxeval = 4e3,
          algorithm = "NLOPT_GN_DIRECT_L_RAND"),
        eval_f = function(x) {
          w[1] * ex2_f1_scaled(x) + w[2] * ex2_f2_scaled(x) + w[3] * ex2_f3_scaled(x)
          #aug_chebyshev(utop_vec = ex2_utop_scaled, w_vec = w, x = x, rho = .001)
        },
        lb = rep(lower, 2),
        ub = rep(upper, 2))
      
      s <- res$solution
      `colnames<-`(
        x = matrix(data = c(ex2_f1(s[1], s[2]), ex2_f2(s[1], s[2]), ex2_f3(s[1], s[2]),
                            ex2_f1_scaled(s), ex2_f2_scaled(s), ex2_f3_scaled(s),
                            w, res$objective, s), nrow = 1),
        value = c("f1", "f2", "f3", "f1s", "f2s", "f3s", "w1", "w2", "w3", "value", "x", "y"))
    }
  }))
  
  # Next, we have to identify the Pareto front, by selecting non-dominated solutions.
  res$optimal <- FALSE
  for (rn in rownames(res)) {
    this_sol <- res[rn, c("f1", "f2", "f3")]
    others <- res[rn != rownames(res), c("f1", "f2", "f3")]
    res[rn, ]$optimal <- !any(others$f1 < this_sol$f1 & others$f2 < this_sol$f2 & others$f3 < this_sol$f3)
  }
  res
})
```


Out of __`r nrow(ex2_pareto_front)`__, a total of __`r sum(ex2_pareto_front$optimal)`__ are Pareto optimal.

```{r echo=FALSE, eval=interactive()}
rgl::plot3d(x = ex2_pareto_front$f1s, y = ex2_pareto_front$f2s, z = ex2_pareto_front$f3s)
```


```{r echo=FALSE, eval=FALSE}
rgl::rgl.snapshot("../results/ex2_pareto_front.png")
```

The 3-dimensional Pareto front is shown in figure \ref{fig:ex2-pareto-front}.

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{../results/ex2_pareto_front.png}
  \caption{The 3-dimensional Pareto front of the Vienna functions (using scaled objectives).}
  \label{fig:ex2-pareto-front}
\end{figure}



# The high-precision ECDFs

We will sample a large number of possible losses from each of the three objectives in order to get an understanding of their distribution.
We thereby establish a bijection between the hyperparameters $\{x,y\}$ (called _decision vector_), and the outcome/objective values (called _objective vector_).
These high-precision ECDFs will be used to map losses to scores. For that, we will use the notion of an ECCDF, the _complementary_ ECDF, which is defined as $1-\operatorname{ECDF}$.
Only with a linear (rectified) understanding of how likely each loss actually is, we can quantify the obtained trade-offs.


In order to obtain these ECDFs (one per objective), we will sample uniformly from the decision space, within its bounds $-3\leq x,y\leq3$.


```{r}
set.seed(1337)
ex2_hp_samples <- matrix(ncol = 2, data = runif(n = 2e6, min = lower, max = upper))
nrow(ex2_hp_samples)
```

```{r}
ex2_high_prec_ecdfs <- loadResultsOrCompute(file = "../results/ex2_high_prec_ecdfs.rds", computeExpr = {
  ecdf_data <- doWithParallelCluster(numCores = 3, expr = {
    library(foreach)
    
    foreach::foreach(
      idx = 1:3,
      .inorder = TRUE,
      .combine = cbind,
      .export = c("ex2_ideal", "ex2_nadir", paste0("ex2_f", 1:3), paste0("ex2_f", 1:3, "_scaled"))
    ) %dopar% {
      obj <- get(paste0("ex2_f", idx, "_scaled"))
      temp <- apply(X = ex2_hp_samples, MARGIN = 1, FUN = function(row) obj(row))
      `colnames<-`(x = matrix(data = temp, ncol = 1), value = paste0("f", idx))
    }
  })
  
  ecdf_data <- as.data.frame(cbind(ex2_hp_samples, ecdf_data))
  colnames(ecdf_data) <- c("x", "y", "f1", "f2", "f3")
  
  # Let's also approximate the ECDFs and return them:
  ecdf_funcs <- unlist(doWithParallelCluster(numCores = 3, expr = {
    library(foreach)
    
    foreach::foreach(
      idx = 1:3,
      .inorder = TRUE,
      .combine = list
    ) %dopar% {
      templ <- list()
      templ[[paste0("ecdf_f", idx)]] <- stats::ecdf(ecdf_data[, paste0("f", idx)])
      templ
    }
  }))
  
  list(
    ecdf_data = ecdf_data,
    ecdf_funcs = ecdf_funcs
  )
})
```


Let's plot the losses' distributions as PDFs and CDFs.
Figure \ref{fig:ex2-high-prec} shows that all three objectives are quite different from each other. It appears that objectives $f_1$ and $f_3$ also follow a somewhat multimodal distribution, while $f_2$ looks like a Log-normal or Gamma distribution.
In any case, none of the distributions is even close to being a standard uniform distribution.


```{r ex2-high-prec, echo=FALSE, fig.cap="The empirical densities and cumulative distributions of the three scaled objectives of the Viennet function.", fig.height=7}
layout(mat = matrix(data = c(1,2,3,4,5,6), ncol = 2, byrow = TRUE))
par(mar=c(5,4,1.5,1), oma = rep(0, 4))

for (i in 1:3) {
  plot(density(ex2_high_prec_ecdfs$ecdf_data[, paste0("f", i)]), main = latex2exp::TeX(paste0("Kernel density of $f_", i, "(x)$")))
  grid()
  
  # Let's first approximate the ECDFs with lower resolution for plotting.
  ext <- range(ex2_high_prec_ecdfs$ecdf_data[, paste0("f", i)])
  x <- seq(from = ext[1], to = ext[2], length.out = 1e3)
  tempf <- stats::approxfun(x = x, y = ex2_high_prec_ecdfs$ecdf_funcs[[paste0("ecdf_f", i)]](x), yleft = 0, yright = 1)
  curve2(func = tempf, from = ext[1], to = ext[2], ylab = latex2exp::TeX("F(x)"), main = latex2exp::TeX(paste0("Cumulative probability of $f_", i, "(x)$")))
  grid()
}
```

## Creating the PPFs

The percent point functions (or quantile functions) can be obtained by inversing a CDF, i.e., $\operatorname{CDF}^{-1}$.

```{r}
make_inverse_ecdf <- function(values, inverse_eccdf = FALSE) {
  e <- ecdf(values)
  y <- e(values)
  if (inverse_eccdf) {
    y <- 1 - y
  }
  stats::approxfun(x = y, y = values, yleft = if (inverse_eccdf) max(values) else min(values), yright = if (inverse_eccdf) min(values) else max(values))
}
```

```{r}
# Those are inverse ECCDFs! -> ECCDF^{-1}
ex2_high_prec_ecdfs$ppf_funcs <- list(
  ppf_f1 = make_inverse_ecdf(ex2_high_prec_ecdfs$ecdf_data$f1, inverse_eccdf = FALSE),
  ppf_f2 = make_inverse_ecdf(ex2_high_prec_ecdfs$ecdf_data$f2, inverse_eccdf = FALSE),
  ppf_f3 = make_inverse_ecdf(ex2_high_prec_ecdfs$ecdf_data$f3, inverse_eccdf = FALSE)
)
```

```{r}
par(mfrow = c(1,3), mar=c(5,4,1.5,1), oma = rep(0, 4))

for (i in 1:3) {
  curve2(func = ex2_high_prec_ecdfs$ppf_funcs[[paste0("ppf_f", i)]], from = 0, to = 1)
  grid()
}
```

## Weight example

Suppose we express a preference using the weights $[0.8, 0.5, 0.2]$. The CDFs and PPFs tell us that each objective's losses are _not_ uniformly distributed.
We can therefore attempt to correct the weights as expressed by the DM, using the following steps.
This only works if all objectives are in the same range.

* Normalize weight vector, such that the largest weight ${=}1$.
* Weights are associated with a certain loss. For example, what loss corresponds to a weight of $0.5$?
* We solve the convex optimization problem for the set of all weights ${<}1$: $\min\,\sum\,(w_i-\operatorname{PPF}_i(x_i))^2$, where $0\leq x_i\leq1$.

In our case this means:

```{r}
ex2_w <- c(0.8, 0.5, 0.2)
(ex2_w <- ex2_w / max(ex2_w))
```

Therefore, the first objective can be ignored, as we want to achieve the minimum possible loss for it anyways.
In other words, even if we now correct the weights, the resulting weight for objective #1 would still be $1$.
We now have to find losses that typically corresponds to $0.625$ and $0.25$ for objectives #2 and #3.

```{r}
res <- nloptr::nloptr(
  x0 = c(0, 0),
  lb = c(0, 0),
  ub = c(1, 1),
  eval_f = function(x) {
    (ex2_w[2] - ex2_high_prec_ecdfs$ppf_funcs$ppf_f2(x[1]))^2 +
    (ex2_w[3] - ex2_high_prec_ecdfs$ppf_funcs$ppf_f3(x[2]))^2
  },
  opts = list(
    maxeval = 1e4,
    algorithm = "NLOPT_GN_DIRECT_L_RAND"),
)
res
```
```{r}
c(
  ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f2(res$solution[1]),
  ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f3(res$solution[2]),
  
  ex2_high_prec_ecdfs$ppf_funcs$ppf_f2(res$solution[1]),
  ex2_high_prec_ecdfs$ppf_funcs$ppf_f3(res$solution[2]))
```

The corrected weight vector therefore is:

```{r}
(w <- c(1, 1-res$solution))
#(w <- c(1, ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f2(1-res$solution[1]), ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f3(1-res$solution[2])))
```

```{r}
res <- nloptr::nloptr(
  x0 = runif(n = 2, min = lower, max = upper),
  opts = list(
    maxeval = 1e5,
    algorithm = "NLOPT_GN_DIRECT_L_RAND"),
  eval_f = function(x) {
    w[1] * ex2_f1_scaled(x) + w[2] * ex2_f2_scaled(x) + w[3] * ex2_f3_scaled(x)
  },
  lb = rep(lower, 2),
  ub = rep(upper, 2))
res
```
```{r}
(temp <- c(
  1-ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f1(ex2_f1_scaled(res$solution)),
  1-ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f2(ex2_f2_scaled(res$solution)),
  1-ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f3(ex2_f3_scaled(res$solution))))
temp / max(temp)
```



Let's gather a large number of results so that we can compare the trade-offs we get with this method.


```{r}
ex2_w_adapt_trade_offs <- doWithParallelCluster(numCores = 15, expr = {
  library(foreach)
  
  foreach::foreach(
    idx = 1:150, #nrow(weight_grid)
    .combine = rbind,
    .inorder = FALSE
  ) %dopar% {
    set.seed(idx)
    w <- as.numeric(weight_grid[idx,])
    
    # The final results will have 12 columns:
    # the original weights, the adapted weights,
    # the objectives' values with either type of weight (original first)
    total_res <- data.frame(
      `colnames<-`(matrix(data = rep(NA, 12), nrow = 1), c(
        paste0("w_org", 1:3),
        paste0("f_org", 1:3),
        paste0("w_opt", 1:3),
        paste0("f_opt", 1:3)
      )))
    total_res[, paste0("w_org", 1:3)] <- w
    
    # First, we do an ordinary optimization and record the results:
    res <- nloptr::nloptr(
      x0 = runif(n = 2, min = lower, max = upper),
      opts = list(
        maxeval = 5e3,
        algorithm = "NLOPT_GN_DIRECT_L_RAND"),
      eval_f = function(x) {
        w[1] * ex2_f1_scaled(x) + w[2] * ex2_f2_scaled(x) + w[3] * ex2_f3_scaled(x)
      },
      lb = rep(lower, 2),
      ub = rep(upper, 2))
    total_res[, paste0("f_org", 1:3)] <- c(
      1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f1(ex2_f1_scaled(res$solution)),
      1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f2(ex2_f2_scaled(res$solution)),
      1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f3(ex2_f3_scaled(res$solution)))
    
    
    
    # Now we gotta optimize the weights first.
    # First, we need to normalize them and find which is the largest.
    w_opt <- w / max(w)
    max_w <- which.max(w_opt)
    objs <- paste0("ppf_f", 1:3)
    objs <- objs[max_w != 1:3]
    
    obj_1 <- ex2_high_prec_ecdfs$ppf_funcs[[objs[1]]]
    obj_2 <- ex2_high_prec_ecdfs$ppf_funcs[[objs[2]]]
    w_1 <- w_opt[w_opt < 1][1]
    w_2 <- w_opt[w_opt < 1][2]
    
    res <- nloptr::nloptr(
      x0 = c(0, 0),
      lb = c(0, 0),
      ub = c(1, 1),
      eval_f = function(x) {
        (1-w_1 - obj_1(x[1]))^2 +
        (1-w_2 - obj_2(x[2]))^2
      },
      opts = list(
        maxeval = 5e3,
        algorithm = "NLOPT_GN_DIRECT_L_RAND"))
    w_new <- rep(NA, 3)
    w_new[max_w] <- 1
    w_new[max_w != 1:3] <- res$solution
    total_res[, paste0("w_opt", 1:3)] <- w_new
    
    
    # Now with the supposedly optimal weights, optimize once more!
    res <- nloptr::nloptr(
      x0 = runif(n = 2, min = lower, max = upper),
      opts = list(
        maxeval = 5e3,
        algorithm = "NLOPT_GN_DIRECT_L_RAND"),
      eval_f = function(x) {
        w_new[1] * ex2_f1_scaled(x) + w_new[2] * ex2_f2_scaled(x) + w_new[3] * ex2_f3_scaled(x)
      },
      lb = rep(lower, 2),
      ub = rep(upper, 2))
    total_res[, paste0("f_opt", 1:3)] <- c(
      1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f1(ex2_f1_scaled(res$solution)),
      1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f2(ex2_f2_scaled(res$solution)),
      1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f3(ex2_f3_scaled(res$solution)))
    
    total_res
  }
})
```

Now let's check out the trade-offs.

```{r}
temp.rmse <- matrix(ncol = 2, nrow = nrow(ex2_w_adapt_trade_offs))

for (idx in 1:nrow(temp.rmse)) {
  row <- ex2_w_adapt_trade_offs[idx,]
  
  w_org <- row[, 1:3]
  w_org <- w_org / max(w_org)
  f_org <- row[, 4:6]
  f_org <- f_org / max(f_org)
  w_opt <- row[, 7:9]
  w_opt <- w_opt / max(w_opt)
  f_opt <- row[, 10:12]
  f_opt <- f_opt / max(f_opt)
  
  temp.rmse[idx,] <- c(
    Metrics::rmse(as.numeric(w_org), as.numeric(f_org)),
    Metrics::rmse(as.numeric(w_opt), as.numeric(f_opt)))
}
```

```{r}
c(
  mean(temp.rmse[,1]),
  mean(temp.rmse[,2]))
```






# Assessing trade-offs of original problem

With the high-precision ECDFs, we have now the means to transforming any loss from the three objectives into a score. After that transformation, we can put the scores into a vector, normalize it, and compare it to the weights which we used to express our preference.

We can reuse the the results from approximating the Pareto front, as we stored the used weights along the results gotten.
Before computing the scores, let's define the ECCDFs for that:

```{r}
ex2_f1_high_prec_eccdf <- function(x) 1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f1(x)
ex2_f2_high_prec_eccdf <- function(x) 1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f2(x)
ex2_f3_high_prec_eccdf <- function(x) 1 - ex2_high_prec_ecdfs$ecdf_funcs$ecdf_f3(x)
```



```{r}
ex2_trade_offs_org <- as.data.frame(cbind(
  ex2_pareto_front[, c("w1", "w2", "w3")],
  data.frame(
    s1 = ex2_f1_high_prec_eccdf(ex2_pareto_front$f1),
    s2 = ex2_f2_high_prec_eccdf(ex2_pareto_front$f2),
    s3 = ex2_f3_high_prec_eccdf(ex2_pareto_front$f3)
  )
))
head(ex2_trade_offs_org)
```

Now we can compare the weight vector against the score vector. For each solution, we will normalize both these vectors.
Then, we perform a nominal-actual comparison for the ratios.
We will compute 3 types of deviation:

* The mean absolute error; e.g., you requested the ratios $[0.5,0.3,0.2]$, but you got $[0.1,0.5,0.4]$. Then the absolute error is $[0.4, 0.2, 0.2]$, and the mean is $\approx0.2667$. It describes by how much each ratio was off.
* The RMSE, which is similar to the MAE.
* An absolute percentual difference. For example, you requested one ratio to be $0.3$, but you got $0.45$. Then the percentual difference for this value is $50$%. If you get $0.15$, the difference is $50$% as well. It is calculated as $\abs{100\times(1-(r^{\mathit{is}}\div r^{\mathit{should}}))}$.


```{r}
ex2_trade_offs_org_diffs <- matrix(ncol = 5, nrow = nrow(ex2_trade_offs_org))

for (i in 1:nrow(ex2_trade_offs_org)) {
  v1 <- as.numeric(ex2_trade_offs_org[i, c("w1", "w2", "w3")])
  v2 <- as.numeric(ex2_trade_offs_org[i, c("s1", "s2", "s3")])
  # Normalize both:
  v1 <- v1 / sum(v1)
  v2 <- v2 / sum(v2)
  
  ex2_trade_offs_org_diffs[i,] <- c(
    Metrics::mae(actual = v2, predicted = v1),
    Metrics::rmse(actual = v2, predicted = v1),
    abs(100 * (1 - v1/v2))
  )
}
```

```{r}
head(ex2_trade_offs_org_diffs)
```
```{r}
temp <- ex2_trade_offs_org_diffs[,]
temp[is.infinite(temp)] <- NA
temp <- temp[complete.cases(temp),]
```


```{r}
par(mfrow = c(1,2))
plot(density(as.numeric(temp[, 3:5])), xlim = c(0, 270))
plot(ecdf(as.numeric(temp[, 3:5])), xlim = c(0, 270))
grid()
```

The mean deviation is __`r round(mean(abs(temp[, 3:5])), 3)`__%.



# Pre-condition the problem

We have seen that getting the desired trade-off is difficult for this problem. We will now attempt to _pre-condition_ the problem by roughly approximating the distributions of each objective's losses.

While we have already high-precision approximations of the ECDFs for each objective, those shall only be used to more accurately test and obtain the score associated with a loss.


```{r}
set.seed(2002)
constellations <- 1e4

prec_weight_grid <- matrix(data = runif(n = constellations * 2, min = lower, max = upper), ncol = 2)
```

```{r}
make_smooth_ecdf <- function(values, slope = 0.025) {
  r <- range(values)
  e <- stats::ecdf(values)
  x <- sort(unique(values))
  y <- e(x)
  if (slope > 0) {
    ext <- r[2] - r[1]
    # Add a sllight slope before and after for numeric stability.
    x <- c(r[1] - ext, x, r[2] + ext)
    y <- c(0 - slope, y, 1 + slope)
  }
  stats::approxfun(x = x, y = y, yleft = y[1], yright = y[length(y)])
}
```


```{r}
ex2_f1_ecdf <- stats::ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f1_scaled(row)))
#ex2_f1_ecdf <- make_smooth_ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f1_scaled(row)))
ex2_f2_ecdf <- stats::ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f2_scaled(row)))
#ex2_f2_ecdf <- make_smooth_ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f2_scaled(row)))
ex2_f3_ecdf <- stats::ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f3_scaled(row)))
#ex2_f3_ecdf <- make_smooth_ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f3_scaled(row)))

ex2_f1_eccdf <- function(x) 1 - ex2_f1_ecdf(x)
ex2_f2_eccdf <- function(x) 1 - ex2_f2_ecdf(x)
ex2_f3_eccdf <- function(x) 1 - ex2_f3_ecdf(x)
```

```{r echo=FALSE}
par(mfrow = c(1,3))

#curve2(func = ex2_f1_eccdf, from = min(ex2_high_prec_ecdfs$ecdf_data$f1), to = max(ex2_high_prec_ecdfs$ecdf_data$f1))
#curve2(func = ex2_f2_eccdf, from = min(ex2_high_prec_ecdfs$ecdf_data$f2), to = max(ex2_high_prec_ecdfs$ecdf_data$f2))
#curve2(func = ex2_f3_eccdf, from = min(ex2_high_prec_ecdfs$ecdf_data$f3), to = max(ex2_high_prec_ecdfs$ecdf_data$f3))
plot(ex2_f1_ecdf, xlim = range(ex2_high_prec_ecdfs$ecdf_data$f1))
plot(ex2_f2_ecdf, xlim = range(ex2_high_prec_ecdfs$ecdf_data$f2))
plot(ex2_f3_ecdf, xlim = range(ex2_high_prec_ecdfs$ecdf_data$f3))
```

The roughly approximated E(C)CDFs look quite similar to the high-precision ones.



## Pareto front using scores

Let's approximate the Pareto front. Again, we will use the augmented Chebyshev scalarizer, but this time we will maximize the scores.
Since each score can maximally be $1$ (or each loss minimally be $0$), we will use these directly as utopian values.


```{r}
# A quick test. Note that we will do minimization and therefore use the ECDFs,
# not the ECCDFs.

set.seed(1)
aug_chebyshev(objectives = list(
  f1 = function(x) {
    ex2_f1_ecdf(ex2_f1_scaled(x))
  },
  f2 = function(x) {
    ex2_f2_ecdf(ex2_f2_scaled(x))
  },
  f3 = function(x) {
    ex2_f3_ecdf(ex2_f3_scaled(x))
  }
), utop_vec = rep(-.1, 3), w_vec = runif(3), x = runif(2, lower, upper))
```

As for the front itself, we will reuse the weight constellations as used originally.
That will also let us compare the fronts then.



```{r echo=FALSE}
ex2_pareto_front_scores <- loadResultsOrCompute(file = "../results/ex2_pareto_front_scores.rds", computeExpr = {
  objectives <- list(
    f1 = function(x) {
      ex2_f1_ecdf(ex2_f1_scaled(x))
    },
    f2 = function(x) {
      ex2_f2_ecdf(ex2_f2_scaled(x))
    },
    f3 = function(x) {
      ex2_f3_ecdf(ex2_f3_scaled(x))
    }
  )
  
  cl <- parallel::makePSOCKcluster(15)
  parallel::clusterExport(cl = cl, varlist = c(
    "objectives",
    "ex2_nadir", "ex2_ideal", "ex2_utop",
    paste0("ex2_f", 1:3), paste0("ex2_f", 1:3, "_ecdf"), paste0("ex2_f", 1:3, "_scaled")))
  
  res <- as.data.frame(doWithParallelClusterExplicit(cl = cl, stopCl = TRUE, expr = {
    library(foreach)
    
    foreach::foreach(
      idx = 1:nrow(weight_grid),
      .combine = rbind,
      .inorder = FALSE
    ) %dopar% {
      w <- as.numeric(weight_grid[idx,])
      set.seed(seed = idx)
      
      res <- nloptr::nloptr(
        x0 = runif(n = 2, min = lower, max = upper),
        opts = list(
          maxeval = 2e3,
          algorithm = "NLOPT_GN_DIRECT_L_RAND"),
        eval_f = function(x) {
          aug_chebyshev(objectives = objectives, utop_vec = c(0,0,0), w_vec = w, x = x, rho = .0001)
        },
        lb = rep(lower, 2),
        ub = rep(upper, 2))
      
      `colnames<-`(
        x = matrix(data = c(ex2_f1_scaled(res$solution), ex2_f2_scaled(res$solution), ex2_f3_scaled(res$solution),
                            w, res$objective, res$solution), nrow = 1),
        value = c("f1", "f2", "f3", "w1", "w2", "w3", "value", "x", "y"))
    }
  }))
  
  # Next, we have to identify the Pareto front, by selecting non-dominated solutions.
  res$optimal <- FALSE
  for (rn in rownames(res)) {
    this_sol <- res[rn, c("f1", "f2", "f3")]
    others <- res[rn != rownames(res), c("f1", "f2", "f3")]
    res[rn, ]$optimal <- !any(others$f1 < this_sol$f1 & others$f2 < this_sol$f2 & others$f3 < this_sol$f3)
  }
  res
})
```

Out of __`r nrow(ex2_pareto_front_scores)`__, a total of __`r sum(ex2_pareto_front_scores$optimal)`__ are Pareto optimal.

```{r}
rgl::plot3d(x = ex2_pareto_front_scores$f1, y = ex2_pareto_front_scores$f2, z = ex2_pareto_front_scores$f3)
```


```{r}
ex2_trade_offs_scores <- as.data.frame(cbind(
  ex2_pareto_front_scores[, c("w1", "w2", "w3")],
  data.frame(
    s1 = ex2_f1_high_prec_eccdf(ex2_pareto_front_scores$f1),
    s2 = ex2_f2_high_prec_eccdf(ex2_pareto_front_scores$f2),
    s3 = ex2_f3_high_prec_eccdf(ex2_pareto_front_scores$f3)
  )
))
head(ex2_trade_offs_scores)
```


```{r}
ex2_trade_offs_scores_diffs <- matrix(ncol = 5, nrow = nrow(ex2_trade_offs_scores))

for (i in 1:nrow(ex2_trade_offs_scores)) {
  v1 <- as.numeric(ex2_trade_offs_scores[i, c("w1", "w2", "w3")])
  v2 <- as.numeric(ex2_trade_offs_scores[i, c("s1", "s2", "s3")])
  # Normalize both:
  v1 <- v1 / sum(v1)
  v2 <- v2 / sum(v2)
  
  ex2_trade_offs_scores_diffs[i,] <- c(
    Metrics::mae(actual = v2, predicted = v1),
    Metrics::rmse(actual = v2, predicted = v1),
    abs(100 * (1 - v1/v2))
  )
}
```

```{r}
head(ex2_trade_offs_scores_diffs)
```

```{r}
temp <- ex2_trade_offs_scores_diffs[,]
temp[is.infinite(temp)] <- NA
temp <- temp[complete.cases(temp),]
```


```{r}
par(mfrow = c(1,2))
plot(density(as.numeric(temp[, 3:5])), xlim = c(0, 270))
plot(ecdf(as.numeric(temp[, 3:5])), xlim = c(0, 270))
grid()
```

The mean deviation is __`r round(mean(abs(temp[, 3:5])), 3)`__%.


## Reference point adjustment

```{r}
check_sol <- function(sol, objectives = list(ex2_f1_scaled, ex2_f2_scaled, ex2_f3_scaled)) {
  objVals <- unlist(lapply(X = objectives, FUN = function(obj) obj(sol)))
  v <- c(
    ex2_f1_high_prec_eccdf(objVals[1]),
    ex2_f2_high_prec_eccdf(objVals[2]),
    ex2_f3_high_prec_eccdf(objVals[3])
  )
  v <- v / max(v)
  v
}
```


```{r}
objectives <- list(ex2_f1_scaled, ex2_f2_scaled, ex2_f3_scaled)
vec_w <- c(1,2,3)

set.seed(1)
res <- nloptr::nloptr(
  x0 = runif(n = 2, min = lower, max = upper),
  opts = list(
    maxeval = 2e3,
    algorithm = "NLOPT_GN_DIRECT_L_RAND"),
  eval_f = function(x) {
    objVals <- unlist(lapply(X = objectives, FUN = function(obj) obj(x)))
    sum(vec_w * (objVals - ex2_utop_scaled)^2)
  },
  lb = rep(lower, 2),
  ub = rep(upper, 2))
```

```{r}
check_sol(res$solution)
#unlist(lapply(X = objectives, FUN = function(obj) obj(res$solution))) - ex2_ideal_scaled
```

Let's map the weights to the reference points.
For that, we need the inverse CDF:


```{r}
ex2_f1_ppf <- make_inverse_ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f1_scaled(row)))
ex2_f2_ppf <- make_inverse_ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f2_scaled(row)))
ex2_f3_ppf <- make_inverse_ecdf(apply(X = prec_weight_grid, MARGIN = 1, FUN = function(row) ex2_f3_scaled(row)))
```

```{r}
par(mfrow = c(1,3))

curve2(ex2_f1_ppf, 0, 1)
curve2(ex2_f2_ppf, 0, 1)
curve2(ex2_f3_ppf, 0, 1)
```


```{r}
(ex2_refpoint <- c(
  ex2_f1_ppf(vec_w[1] / max(vec_w)),
  ex2_f2_ppf(vec_w[2] / max(vec_w)),
  ex2_f3_ppf(vec_w[3] / max(vec_w))
))
```

Now let's use this as a reference point without weights:

```{r}
set.seed(1)
res <- nloptr::nloptr(
  x0 = runif(n = 2, min = lower, max = upper),
  opts = list(
    maxeval = 2e3,
    algorithm = "NLOPT_GN_DIRECT_L_RAND"),
  eval_f = function(x) {
    objVals <- unlist(lapply(X = objectives, FUN = function(obj) obj(x)))
    sum(abs((objVals - ex2_refpoint)^2))
  },
  lb = rep(lower, 2),
  ub = rep(upper, 2))
```

```{r}
unlist(lapply(X = objectives, FUN = function(obj) obj(res$solution)))
```




```{r}
set.seed(6)
#(vec_temp <- vec_w / (max(vec_w)))
vec_temp <- runif(3)
(vec_temp <- vec_temp / max(vec_temp))
```


```{r}
(temp <- ex2_nadir_scaled - vec_temp * (ex2_nadir_scaled - ex2_ideal_scaled))
```

Now we have to pass this vector into the ECDFs to get the real weights!

```{r}
(vec_w_real <- c(
  ex2_f1_eccdf(temp[1]),
  ex2_f2_eccdf(temp[2]),
  ex2_f3_eccdf(temp[3])
))
```


```{r}
set.seed(1)
res <- nloptr::nloptr(
  x0 = runif(n = 2, min = lower, max = upper),
  opts = list(
    maxeval = 2e3,
    algorithm = "NLOPT_GN_DIRECT_L_RAND"),
  eval_f = function(x) {
    objVals <- unlist(lapply(X = objectives, FUN = function(obj) obj(x)))
    sum(vec_w_real * (objVals - ex2_utop_scaled)^2)
  },
  lb = rep(lower, 2),
  ub = rep(upper, 2))
```

```{r}
check_sol(res$solution)
#unlist(lapply(X = objectives, FUN = function(obj) obj(res$solution))) - ex2_ideal_scaled
```




\clearpage

# References {-}

<div id="refs"></div>





